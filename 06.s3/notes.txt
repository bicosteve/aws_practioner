Introduction 
- amazon s3 is one of the building blocks of aws.
- it is advertised as infinitely scaling storage 
- many websites use s3 as a backbone 
- many aws services uses s3 as an integration as well 
- 

S3 Use Cases 
- back and storage 
- disaster recovery 
- archive data 
- hybrid cloud storage 
- appplication hosting 
- media hosting 
- data lakes and big data analytics 
- software delivery 
- static websites 

S3 Overview - Buckets 
- s3 allows people to store `objects` (files) in `buckets` (directories).
- buckets must have globally unique name (across all regions and accounts)
- buckets are define at region level 
- s3 looks like a global service but the buckets are created in a region 
- naming convention is: 
    -> no uppercase 
    -> no underscore 
    -> 3-36 characters long 
    -> Not an IP 
    -> must start with lowercase letter or number 

S3 Overview - Objects 
- objects (files) have a `Key` 
- the key is the FULL path : eg s3://my-bucket/my_file.txt here, key is my_file.txt . Can also be in subfolders eg s3://my-bucket/my_folder1/another_folder/my_file.txt 
- the key is composed of prefix + object name 
    -> s3://my-bucket/my_folder1/another_folder/my_file.txt 
- object values are the content of the body 
- max object size is 5TB 
- if using over 5GB must use multi-part upload 
- has metadata (list of text of key/value pairs - system or user metadata)
- has tags which are useful for security/lifecycle 


S3 Security Bucket Policy 

(a) User Based 
- Uses IAM policies that is, which API calls should be allowed for a specific user from IAM console 

(b) Resource Based 
-> bucket policies : bucket wide rules from the S3 console - allows cross account 
-> object access control list (ACL) : finer grain 
-> bucket access control list (ACL) : less common 

NB: an IAM principal can access an S3 object if:
    -> the user IAM permission allows it OR the resource policy ALLOWS it 
    -> there is no explicit DENY on the bucket 
- you can encrypt objects using encryption keys so that no one but you can decrypt the objects 

** Example One : Public Access - Use Bucket Policy 
- if an anonymous user from a website tries to access your S3 bucket, create a S3 Bucket Policy which allows public access to the bucket 

** Example Two : User Access to S3 - IAM Permission 
- Create an IAM Policy which allows user to access the bucket 
- You do not need S3 Bucket Policy since the IAM policy allows user to access the bucket 

** Example Three : EC2 instance access - Use IAM Roles 
- create an EC2 instance Role and attach IAM permissions to the ec2 instance role.
- the ec2 instance will use the role to access the bucket 

** Example Four : Cross-Account Access - Use Bucket Policy 



S3 Bucket Policies 
- they are json based policies
- resource based : buckets and objects 
- actions : set of api to allow or deny 
- effect : allow/deny  
{
    "Version" : "2012-10-17",
    "Statement" : [
        {
            "Sid":"PublicRead", 
            "Effect":"Allow", 
            "Principal":"*",
            "Action" : [
                "s3:GetObject"
            ],
            "Resource" : [
                "arn:aws:s3:::examplebucket/*"
            ]
        }
    ]

}

- use s3 bucket for policy to :
    -> grant public access to the bucket 
    -> force object to be encrypted at upload 
    -> grant access to antoher account (cross account)
    -> NB: Leave Block Public Access ON if you know your data should not be public 
    -> they were created to prevent company data leaks 


S3 Websites 
- s3 can host static websites and have them accessible via www 
- the website url will be : <bucket-name>.s3-website-<AWS-region>.amazonaws.com 
- you should be able to access the url once the bucket is public 
- enable the bucket as a website with html files 
- check on properties -> static website hosting
- you will get the bucket website endpoint 

S3 Versioning 
- you can version your files in s3 
- it is enabled at the bucket level 
- same key overwrite with increment the 'version' 1,2,3 
- it is best practice to version your buckets 
- versioning protects against intended deletes (ability to restore version)
- easy roll back to previous version 

NB: 
-> any file that is not versioned prior to enabling versioning will have version 'null'
-> suspending versioning does not delete the previous versions 


S3 Access Logs 
- for audit purpose, you may want to log all access to your S3 buckets 
- any request made to S3 from any account, authorized or denied, will be logged into another S3 bucket 
- that data can be analyzed using data analysis tools 
- very helpful to come down to the root case of an issue 
- click on the bucket -> settings -> server access logging 


S3 Replication (CRR & SRR)
- you can have replication of your bucket across regions 
- it is done asynchronously 
- you must enable versioning in source and destination 
- Cross Region Replication (CRR)
- Same Region Replication (SRR)
- buckets can be in different accounts 
- copying is asynchronous
- must give proper IAM permission to S3 

-> CRR Use Case : compliance, lower latency access, replication across accounts 
-> SRR Use Case : log aggregation, live replication between production and test accounts 


S3 Storage Classes 
1. General Purpose S3 which is standard
2. Standard-Infrequent Access (IA) : not frequently accessed data 
3. One Zone-Infrequenct Acces : File which you do not access often 
4. Intelligent Tiering : Optimizes cost based on data frequency access 
5. Glacier : Designed for long term low cost data storage. Perfect for rarely accessed data.
6. Glacier Deep Archive : lowest-cost storage class, purpose-built for long-term data retention where access is extremely rareâ€”think once a year or less.

** Terms ** 
-> Durability : how often you will lose a file. High durability has 11 9's -> 99.999999999% 
-> same for all storage classe 
-> Availability : measures how readily available a service is. S3 standard is 99.99% availability meaning in a year, it will only be unavailable for 53 minutes! 

S3 Storage Classes Deep Dive 
1. S3 Standard - General Purpose 
- is available 99.99% 
- used for frequently accessed data 
- give low latency and high throughput 
- sustain 2 concurrent facility failures 
- Use Cases : big data analytics, mobile & gaming applications, content distribution etc 

2. S3 Standard - Infrequent Access 
- suitable for data less frequently accessed but when you want it you want it fast 
- 99.9% 
- lower cost 
- when files are accessed you incur a fee 
- sustain 2 concurrent facility failures 
- Use Case : as a data store for disaster recovery, backups ...

3. S3 Intelligent-Tiering 
- 99.9% availability 
- same low latency and high throughput performance of S3 standard 
- cost-optimized by automatically moving objects between two access tiers based on changing access patterns:
    -> frequent access : 
    -> infrequent access :
- reslient against events that will impact an entire availability zone 


4. S3 One Zone - Infrequent Access (IA)
- same as IA but data is stored in a single zone 
- less availability of 99.5% 
- lower cost compared to S3-IA 
- Use Case : Storing secondary backup copies of on-premise data or storing data you can recreate 


5. Amazon Glacier & Glacier Deep Archive 
- something you will use for archiving backup
- data will be retained for longer time 
- glacier is cheap 
- glacier deep archive even cheaper 


6. AWS Snow Family
- highly secure, portable device to collect and process data at the edge and to migrate data in and out of AWS 
- These are the Snow Family 
    1. Migrating Data 
        - snowcone 
        - snowball edge 
        - snow mobile 

    2. Edge Computing 
        - snowcone 
        - snowball edge 


Data Migration with Snow Family 
- there can be challenges in transferring data due to limited connectivity, limited bandwidth, shared bandwidth, and high network cost. 
- all these challenges make the case for the AWS Snow Family 
- they are offline devices that perform data migrations 
- if it takes more than one week to transfer data, use snowball devices 
- for data migration we have snowcone, snowball edge, and snowmobile 


How to use Snow Family 
- request for snowball devices from the AWS console for delivery 
- install snowball client / AWS OpsHub 
- ship back the device when you are done 
- data will be loaded to s3  bucket 
- snowball is completely wiped 

Edge Computing 
- processing data while it is being created on an edge location 
- can be a truck on the road, ship, mining station underground 
- these location can be limited in terms of internet access, no easy acces to computing power 


Hybrid Cloud for Storage 
- aws is pushing for hybrid cloud 
- part of your infra is on prem 
- part of your infra is on cloud 
- benefits are :
    long cloud migrations 
    security requirements 
    compliance requirements 
    IT strategy 
BN: S3 is a proprietary storage technology therefore you need to expose S3 data on prem 
- use a `Storage Gateway` which bridges on premise data and cloud data on S3 
- hybrid sotrage service allows on-prem to seamlessly use the AWS cloud 
- use cases : disaster recovery, backup, & restore, tiered storage 

Types of storage gateway :
- file gateway 
- volume gateway 
- tape getway 



